<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[指数加权平均]]></title>
    <url>%2F2017%2F12%2F09%2F%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87%2F</url>
    <content type="text"><![CDATA[1. 计算方法 对一个序列 $ _1,_2,_3 ... _n $ ,引入参数 \(\beta\) ，使用如下方法可以计算得到一个新的序列 \(v\) : \(\forall t \in (1,n)\) \(v_t = \beta v_{t-1}+(1-\beta)\theta_t\) 这个新的序列 $ v_1,v_2,v_3 ... v_n $ 的每个元素 \(v_t\) 可以近似看做是原序列截止到第 \(t\) 个元素为止，前 \(\frac{1}{1-\beta}\) 个元素的加权平均。那么，为什么可以这样近似？ 2. 原理解释 将迭代式子展开，可以得到： \(v_t=(1-\beta) (\theta_t+\beta\theta_{t-1} +\beta^2\theta_{t-2}+\beta^3\theta_{t-3}+\beta^4\theta_{t-4}+\cdots)\) 因为 \(\beta\) 是小于1的数，所以随着指数的增大，越趋近于0，而一般认为当指数项衰减到 \(\frac{1}{e}\) 就可以忽略不计。所以，只需要证明 \[\beta^{\frac{1}{1-\beta}}=\frac{1}{e}\] 成立即可。 \[\beta^{\frac{1}{1-\beta}}=\frac{1}{e}\] \[\begin{eqnarray} &amp;\Leftrightarrow&amp; \frac{\ln\beta}{1-\beta}=-1 \\ &amp;\Leftrightarrow&amp; {\ln\beta}=\beta-1 \\ &amp;\Leftrightarrow&amp; \ln(\beta+1)=\beta \end{eqnarray}\] 即为 \(\ln(x+1)\) 在 \(x=0\) 处的一阶泰勒展开，所以原等式成立。 常用的值有： \(\beta=0.9\) 时，\(\beta^{10}=\frac{1}{e}\) \(\beta=0.98\) 时，\(\beta^{50}=\frac{1}{e}\) 3. 偏差修正 为了修正指数加权平均在前期的误差，将迭代公式修改为： \[v_t = \frac{(\beta v_{t-1}+(1-\beta)\theta_t)}{(1-\beta^t)}\] 当然，这种偏差只会在前期存在，如果不在意前期的这点误差，完全可以忽略。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>optimization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Batch Norm]]></title>
    <url>%2F2017%2F12%2F09%2FBatch-Norm%2F</url>
    <content type="text"><![CDATA[本文主要是在看完Andrew Ng的Deep Learning Specialization（Coursera链接，网易云课堂官方授权搬运）系列中相关课程后的记录整理和一点思考 1. 什么是Batch-Norm 首先，Batch-Norm（以下简称BN）是一种归一化的方法，不仅可以在深度学习中使用，在很多其他统计学习（比如Logistic Regression）中也适用。归一化的目标都是希望能加速学习过程，一个简单的例子就是：在使用梯度下降求解最优解的时候，如果等高线（或者说等值线？）的形状越近似于圆，算法收敛到最优解的速度越快，而BN就是将原本的椭圆处理成近似圆。 2. 神经网络中Batch-Norm的实现 BN实现过程如下： \[ \mu=\frac{1}{m}\sum_i{z^{(i)}} ​\] \[\sigma^2=\frac{1}{m}\sum_i(z^{(i)}-\mu)^2\] \[ z_{norm}^{(i)}=\frac{z^{(i)-\mu}}{\sqrt{(\sigma^2+\epsilon)}} \] \[ \tilde{z}^{(i)}=\gamma*z_{norm}^{(i)}+\beta \] 其实就是简单的“减均值除方差”归一化，不过在“除方差”的时候加上了一个 \(\epsilon\) ，是为了在方差近似等于0的时候能保持数值计算稳定。这样就得到的所有 \(z^{(i)}\) 都是均值为0，方差为1的分布，但是我们并不希望每个隐藏单元的输入都是这样，因此有了最后一行的\(\gamma\) 和 $ $ ，作用是可以设置 \(\tilde{z}\) 的平均值。注意：这是两个需要在学习过程中不断学习调整的参数，可以用比如Adam之类的学习算法调节到其最优值。 BN的使用对象是所有的 \(z\) ，就是隐层神经元的输入，在使用激活函数之前对其进行归一化，发生在$ z a$ 的过程之前。本来需要用学习算法更新值的是 $ w,b $ ，而由于 $ b $ 只影响均值，但是最终 \(z\) 的均值取决于 \(\gamma\) 和 $ $ 这两个参数，所以 $ b $ 的作用会被 \(\gamma\) 和 $ $ 消除掉。因此，选择不进行 $ b $ 参数的更新 ，或者说，也就不存在 $ b $ 参数了。 3. 实际模型训练中使用Batch-Norm 对于大规模的数据，完全加载到内存中是不可能的，因此，和mini-batch gradient descent一样，也是将整个数据集分成很多个小规模的mini-batch。归一化的过程是针对每个mini-batch，对应公式中的均值和方差也是每个mini-batch的均值和方差。每个mini-batch可以看做是从数据集整体中进行抽样得到的，因此其均值和方差相对整个数据集来说会有一定的差异，这相当于对数据集加上了一些噪声，也就因此会有一点点（真的只有那么一点点）正则化的效果。需要注意的是：由于效果微乎其微，正则化只能作为BN的一点意外收获，并不能完全取代比如Dropout等“专业正则化”方法。 4. 测试时候的Batch-Norm 因为在测试的时候，模型是针对每个单独的样本输入预测其对应的label，而这个时候计算单个样本的均值和方差显然是没有意义的，所以需要一个方法来估算mini-batch的均值和方差，就可以用指数加权平均（exponentiallly weighted average）。当然，在深度学习的框架（比如tensorflow）中会有默认的估算方式。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>coursera</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Numpy进行one-hot编码]]></title>
    <url>%2F2017%2F12%2F08%2F%E4%BD%BF%E7%94%A8Numpy%E8%BF%9B%E8%A1%8Cone-hot%E7%BC%96%E7%A0%81%2F</url>
    <content type="text"><![CDATA[在实现很多机器学习任务的时候，经常需要将labels进行one-hot encoding，具体思想这里就不详述，借一张图来表示： 由于最后的每个label向量只有一个维度的值是1，其他都是0，所以实现方法可以借助线性代数中的单位矩阵 [百度百科] [wikipedia] Numpy实现可以是这样： 1234567891011121314# 函数需不需要返回转置要根据具体情况看# 如果不转置每个label返回的就是一个行向量# 这里转置了，每个label就是对应的列向量def convert_to_one_hot(y, C): return np.eye(C)[y.reshape(-1)].T y = np.array([1,2,3,4])convert_to_one_hot(y,5)# array([[ 0., 0., 0., 0.],# [ 1., 0., 0., 0.],# [ 0., 1., 0., 0.],# [ 0., 0., 1., 0.],# [ 0., 0., 0., 1.]])]]></content>
      <categories>
        <category>代码</category>
      </categories>
      <tags>
        <tag>Numpy</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy中b=a.reshpe(-1)]]></title>
    <url>%2F2017%2F12%2F08%2Fnumpy%E4%B8%ADb-a-reshpe-1%2F</url>
    <content type="text"><![CDATA[在Python中，-1可以表示最后一个元素，可是Numpy的reshape成-1的形状是什么意思？注意，这里 并不是取最后一个维度！ 并不是取最后一个维度！ 并不是取最后一个维度！ 其实原因很简单，如果是最后一个维度，那得到的这个新数组的长度和之前的对不上。 那 -1 代表什么？我们来看Numpy文档的介绍： newshape : int or tuple of ints The new shape should be compatible with the original shape. If an integer, then the result will be a 1-D array of that length. One shape dimension can be -1. In this case, the value is inferred from the length of the array and remaining dimensions. 介绍得很清楚了，-1 在这里的作用是占一个位，表示这里有一个维度，那么具体是多少回由Numpy计算出来之后再填上去。 来个例子帮助理解： 1234567import Numpy as npa = np.array([ [[[3,2],[3,2]],[3,2],[3,2]],[[[3,2],[3,2]],[3,2],[3,2]],[[[3,2],[3,2]],[3,2],[3,2]] ])print(a.shape)# (3, 3, 2)b = a.reshape(a.shape[0],-1)print(b.shape)# (3, 6)]]></content>
      <categories>
        <category>代码</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DeepLearning-Ng编程作业踩过的一些坑]]></title>
    <url>%2F2017%2F12%2F07%2FDeepLearning-Ng%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A%E8%B8%A9%E8%BF%87%E7%9A%84%E4%B8%80%E4%BA%9B%E5%9D%91%2F</url>
    <content type="text"><![CDATA[1.Course 1 Assignment 3 error 1 问题代码： 12# Visualize the data:plt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral); 执行后报错： ValueError: c of shape (1, 400) not acceptable as a color sequence for x with size 400, y with size 400 需要将上面的代码修改如下： 12# Visualize the data:plt.scatter(X[0, :], X[1, :], c=Y.flatten(), s=40, cmap=plt.cm.Spectral); 然后就可以看到这朵花了 Assignment 3 error 2 问题代码： 1234X_assess, parameters = forward_propagation_test_case()A2, cache = forward_propagation(X_assess, parameters)# Note: we use the mean here just to make sure that your output matches ours. print(np.mean(cache['Z1']) ,np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2'])) 执行后报错： ipykernel_launcher.py:20: RuntimeWarning: divide by zero encountered in log ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in add 检查你的计算前馈网络时候使用的激活函数，将forward_propagation函数的计算A2的代码修改如下： 1234Z1 = np.dot(W1,X)+b1A1 = np.tanh(Z1)Z2 = np.dot(W2,A1)+b2A2 = sigmoid(Z2) 并且注意，如果这里得不到一致的结果，直接会影响最后的nn_model函数，构建不出恰当的模型 2.Course 2 Assignment 1 error 1 可能会在很多地方看到类似这样的报错 ValueError: c of shape (1, 300) not acceptable as a color sequence for x with size 300, y with size 300 这是由于提供的代码中，很多地方都是plt.scatter函数的参数 c 出问题，下面不一一列举，简述个人遇到的需要修改的地方： 修改reg_utils.py: 1234# 324行plt.scatter(X[0, :], X[1, :], c=np.squeeze(y), cmap=plt.cm.Spectral)# 334行plt.scatter(train_X[0, :], train_X[1, :], c=np.squeeze(train_Y), s=40, cmap=plt.cm.Spectral); 3. Course 3 test test2 4. Course 4]]></content>
      <categories>
        <category>代码</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>coursera</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy中sum函数求和结果维度问题]]></title>
    <url>%2F2017%2F12%2F05%2Fnumpy%E4%B8%ADsum%E5%87%BD%E6%95%B0%E6%B1%82%E5%92%8C%E7%BB%93%E6%9E%9C%E7%BB%B4%E5%BA%A6%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[使用Numpy（下面简称np）中的sum函数对某一维度求和时，由于该维度会在求和后变成一个数，所以所得结果的这一维度为空。比如下面的例子： 1234a = np.array([[1,2,3],[4,5,6]])b = np.sum(a,axis=1)print(b.shape)# (2,) 所以，对于一个shape为(2,3)的数组，在默认情况下使用np.sum函数求和后得到的结果shape是 (2,)，如果我们想得到的是(2,1)的shape怎么办？比如Ng的深度学习编程练习中Course 1 Assignment 4就要求这样。使用reshape函数当然可以，只是没有必要，太麻烦了一点不优雅。我们可以使用通过设置keepdims参数实现，还是这个例子： 1234a = np.array([[1,2,3],[4,5,6]])b = np.sum(a,axis=1,keepdims=True)print(b.shape)# (2,1) (2,1)和(2,)的shape之间不同参见 What's the difference between (N,) and (N,1) in Numpy? ---Stackoverflow 这里有个小例子可以帮助理解： 1234567891011a = np.ones((5,))b = np.ones((5,1))print(a)# [1. 1. 1. 1. 1.]print(b)# [[1.]# [1.]# [1.]# [1.]# [1.]]]]></content>
      <categories>
        <category>代码</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
        <tag>python</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
</search>
